{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88668224",
   "metadata": {},
   "source": [
    "# Google 10 years StockPrice Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acf92530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d14884",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"GoogleStock Price Updated.csv\") #https://www.kaggle.com/datasets/jillanisofttech/google-10-years-stockprice-dataset?select=GoogleStock+Price.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91401043",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37ce33f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013/01/02</td>\n",
       "      <td>18.003504</td>\n",
       "      <td>18.193193</td>\n",
       "      <td>17.931683</td>\n",
       "      <td>18.099348</td>\n",
       "      <td>18.099348</td>\n",
       "      <td>101550348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013/01/03</td>\n",
       "      <td>18.141392</td>\n",
       "      <td>18.316566</td>\n",
       "      <td>18.036036</td>\n",
       "      <td>18.109859</td>\n",
       "      <td>18.109859</td>\n",
       "      <td>92635272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2013/01/04</td>\n",
       "      <td>18.251753</td>\n",
       "      <td>18.555305</td>\n",
       "      <td>18.210211</td>\n",
       "      <td>18.467718</td>\n",
       "      <td>18.467718</td>\n",
       "      <td>110429460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2013/01/07</td>\n",
       "      <td>18.404655</td>\n",
       "      <td>18.503002</td>\n",
       "      <td>18.282784</td>\n",
       "      <td>18.387136</td>\n",
       "      <td>18.387136</td>\n",
       "      <td>66161772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2013/01/08</td>\n",
       "      <td>18.406906</td>\n",
       "      <td>18.425926</td>\n",
       "      <td>18.128880</td>\n",
       "      <td>18.350851</td>\n",
       "      <td>18.350851</td>\n",
       "      <td>66976956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>2531</td>\n",
       "      <td>2023/01/23</td>\n",
       "      <td>97.949997</td>\n",
       "      <td>100.040001</td>\n",
       "      <td>97.500000</td>\n",
       "      <td>99.790001</td>\n",
       "      <td>99.790001</td>\n",
       "      <td>40005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>2532</td>\n",
       "      <td>2023/01/24</td>\n",
       "      <td>98.099998</td>\n",
       "      <td>99.610001</td>\n",
       "      <td>97.199997</td>\n",
       "      <td>97.699997</td>\n",
       "      <td>97.699997</td>\n",
       "      <td>33078500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533</th>\n",
       "      <td>2533</td>\n",
       "      <td>2023/01/25</td>\n",
       "      <td>95.570000</td>\n",
       "      <td>96.160004</td>\n",
       "      <td>93.760002</td>\n",
       "      <td>95.220001</td>\n",
       "      <td>95.220001</td>\n",
       "      <td>42330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>2534</td>\n",
       "      <td>2023/01/26</td>\n",
       "      <td>96.500000</td>\n",
       "      <td>97.570000</td>\n",
       "      <td>95.379997</td>\n",
       "      <td>97.519997</td>\n",
       "      <td>97.519997</td>\n",
       "      <td>30114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535</th>\n",
       "      <td>2535</td>\n",
       "      <td>2023/01/27</td>\n",
       "      <td>97.309998</td>\n",
       "      <td>100.320000</td>\n",
       "      <td>97.309998</td>\n",
       "      <td>99.370003</td>\n",
       "      <td>99.370003</td>\n",
       "      <td>33850200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2536 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0        Date       Open        High        Low      Close  \\\n",
       "0              0  2013/01/02  18.003504   18.193193  17.931683  18.099348   \n",
       "1              1  2013/01/03  18.141392   18.316566  18.036036  18.109859   \n",
       "2              2  2013/01/04  18.251753   18.555305  18.210211  18.467718   \n",
       "3              3  2013/01/07  18.404655   18.503002  18.282784  18.387136   \n",
       "4              4  2013/01/08  18.406906   18.425926  18.128880  18.350851   \n",
       "...          ...         ...        ...         ...        ...        ...   \n",
       "2531        2531  2023/01/23  97.949997  100.040001  97.500000  99.790001   \n",
       "2532        2532  2023/01/24  98.099998   99.610001  97.199997  97.699997   \n",
       "2533        2533  2023/01/25  95.570000   96.160004  93.760002  95.220001   \n",
       "2534        2534  2023/01/26  96.500000   97.570000  95.379997  97.519997   \n",
       "2535        2535  2023/01/27  97.309998  100.320000  97.309998  99.370003   \n",
       "\n",
       "      Adj Close     Volume  \n",
       "0     18.099348  101550348  \n",
       "1     18.109859   92635272  \n",
       "2     18.467718  110429460  \n",
       "3     18.387136   66161772  \n",
       "4     18.350851   66976956  \n",
       "...         ...        ...  \n",
       "2531  99.790001   40005100  \n",
       "2532  97.699997   33078500  \n",
       "2533  95.220001   42330000  \n",
       "2534  97.519997   30114000  \n",
       "2535  99.370003   33850200  \n",
       "\n",
       "[2536 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad86d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df[\"Unnamed: 0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9056734e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013/01/02</td>\n",
       "      <td>18.003504</td>\n",
       "      <td>18.193193</td>\n",
       "      <td>17.931683</td>\n",
       "      <td>18.099348</td>\n",
       "      <td>18.099348</td>\n",
       "      <td>101550348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013/01/03</td>\n",
       "      <td>18.141392</td>\n",
       "      <td>18.316566</td>\n",
       "      <td>18.036036</td>\n",
       "      <td>18.109859</td>\n",
       "      <td>18.109859</td>\n",
       "      <td>92635272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013/01/04</td>\n",
       "      <td>18.251753</td>\n",
       "      <td>18.555305</td>\n",
       "      <td>18.210211</td>\n",
       "      <td>18.467718</td>\n",
       "      <td>18.467718</td>\n",
       "      <td>110429460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013/01/07</td>\n",
       "      <td>18.404655</td>\n",
       "      <td>18.503002</td>\n",
       "      <td>18.282784</td>\n",
       "      <td>18.387136</td>\n",
       "      <td>18.387136</td>\n",
       "      <td>66161772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013/01/08</td>\n",
       "      <td>18.406906</td>\n",
       "      <td>18.425926</td>\n",
       "      <td>18.128880</td>\n",
       "      <td>18.350851</td>\n",
       "      <td>18.350851</td>\n",
       "      <td>66976956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>2023/01/23</td>\n",
       "      <td>97.949997</td>\n",
       "      <td>100.040001</td>\n",
       "      <td>97.500000</td>\n",
       "      <td>99.790001</td>\n",
       "      <td>99.790001</td>\n",
       "      <td>40005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>2023/01/24</td>\n",
       "      <td>98.099998</td>\n",
       "      <td>99.610001</td>\n",
       "      <td>97.199997</td>\n",
       "      <td>97.699997</td>\n",
       "      <td>97.699997</td>\n",
       "      <td>33078500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533</th>\n",
       "      <td>2023/01/25</td>\n",
       "      <td>95.570000</td>\n",
       "      <td>96.160004</td>\n",
       "      <td>93.760002</td>\n",
       "      <td>95.220001</td>\n",
       "      <td>95.220001</td>\n",
       "      <td>42330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>2023/01/26</td>\n",
       "      <td>96.500000</td>\n",
       "      <td>97.570000</td>\n",
       "      <td>95.379997</td>\n",
       "      <td>97.519997</td>\n",
       "      <td>97.519997</td>\n",
       "      <td>30114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535</th>\n",
       "      <td>2023/01/27</td>\n",
       "      <td>97.309998</td>\n",
       "      <td>100.320000</td>\n",
       "      <td>97.309998</td>\n",
       "      <td>99.370003</td>\n",
       "      <td>99.370003</td>\n",
       "      <td>33850200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2536 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       Open        High        Low      Close  Adj Close  \\\n",
       "0     2013/01/02  18.003504   18.193193  17.931683  18.099348  18.099348   \n",
       "1     2013/01/03  18.141392   18.316566  18.036036  18.109859  18.109859   \n",
       "2     2013/01/04  18.251753   18.555305  18.210211  18.467718  18.467718   \n",
       "3     2013/01/07  18.404655   18.503002  18.282784  18.387136  18.387136   \n",
       "4     2013/01/08  18.406906   18.425926  18.128880  18.350851  18.350851   \n",
       "...          ...        ...         ...        ...        ...        ...   \n",
       "2531  2023/01/23  97.949997  100.040001  97.500000  99.790001  99.790001   \n",
       "2532  2023/01/24  98.099998   99.610001  97.199997  97.699997  97.699997   \n",
       "2533  2023/01/25  95.570000   96.160004  93.760002  95.220001  95.220001   \n",
       "2534  2023/01/26  96.500000   97.570000  95.379997  97.519997  97.519997   \n",
       "2535  2023/01/27  97.309998  100.320000  97.309998  99.370003  99.370003   \n",
       "\n",
       "         Volume  \n",
       "0     101550348  \n",
       "1      92635272  \n",
       "2     110429460  \n",
       "3      66161772  \n",
       "4      66976956  \n",
       "...         ...  \n",
       "2531   40005100  \n",
       "2532   33078500  \n",
       "2533   42330000  \n",
       "2534   30114000  \n",
       "2535   33850200  \n",
       "\n",
       "[2536 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ce25862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2536 entries, 0 to 2535\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Date       2536 non-null   object \n",
      " 1   Open       2536 non-null   float64\n",
      " 2   High       2536 non-null   float64\n",
      " 3   Low        2536 non-null   float64\n",
      " 4   Close      2536 non-null   float64\n",
      " 5   Adj Close  2536 non-null   float64\n",
      " 6   Volume     2536 non-null   int64  \n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 138.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cae0b5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         0\n",
       "Open         0\n",
       "High         0\n",
       "Low          0\n",
       "Close        0\n",
       "Adj Close    0\n",
       "Volume       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() #We examine the empty lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9ec2e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"]=pd.to_datetime(df[\"Date\"])\n",
    "df[\"day\"]=(df[\"Date\"]).dt.day\n",
    "df[\"month\"]=(df[\"Date\"]).dt.month\n",
    "df[\"year\"]=(df[\"Date\"]).dt.year\n",
    "#We divide date into 3 as day, month and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1189de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df[\"Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "382e13cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.003504</td>\n",
       "      <td>18.193193</td>\n",
       "      <td>17.931683</td>\n",
       "      <td>18.099348</td>\n",
       "      <td>18.099348</td>\n",
       "      <td>101550348</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.141392</td>\n",
       "      <td>18.316566</td>\n",
       "      <td>18.036036</td>\n",
       "      <td>18.109859</td>\n",
       "      <td>18.109859</td>\n",
       "      <td>92635272</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.251753</td>\n",
       "      <td>18.555305</td>\n",
       "      <td>18.210211</td>\n",
       "      <td>18.467718</td>\n",
       "      <td>18.467718</td>\n",
       "      <td>110429460</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.404655</td>\n",
       "      <td>18.503002</td>\n",
       "      <td>18.282784</td>\n",
       "      <td>18.387136</td>\n",
       "      <td>18.387136</td>\n",
       "      <td>66161772</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.406906</td>\n",
       "      <td>18.425926</td>\n",
       "      <td>18.128880</td>\n",
       "      <td>18.350851</td>\n",
       "      <td>18.350851</td>\n",
       "      <td>66976956</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>97.949997</td>\n",
       "      <td>100.040001</td>\n",
       "      <td>97.500000</td>\n",
       "      <td>99.790001</td>\n",
       "      <td>99.790001</td>\n",
       "      <td>40005100</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>98.099998</td>\n",
       "      <td>99.610001</td>\n",
       "      <td>97.199997</td>\n",
       "      <td>97.699997</td>\n",
       "      <td>97.699997</td>\n",
       "      <td>33078500</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533</th>\n",
       "      <td>95.570000</td>\n",
       "      <td>96.160004</td>\n",
       "      <td>93.760002</td>\n",
       "      <td>95.220001</td>\n",
       "      <td>95.220001</td>\n",
       "      <td>42330000</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>96.500000</td>\n",
       "      <td>97.570000</td>\n",
       "      <td>95.379997</td>\n",
       "      <td>97.519997</td>\n",
       "      <td>97.519997</td>\n",
       "      <td>30114000</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535</th>\n",
       "      <td>97.309998</td>\n",
       "      <td>100.320000</td>\n",
       "      <td>97.309998</td>\n",
       "      <td>99.370003</td>\n",
       "      <td>99.370003</td>\n",
       "      <td>33850200</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2536 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Open        High        Low      Close  Adj Close     Volume  day  \\\n",
       "0     18.003504   18.193193  17.931683  18.099348  18.099348  101550348    2   \n",
       "1     18.141392   18.316566  18.036036  18.109859  18.109859   92635272    3   \n",
       "2     18.251753   18.555305  18.210211  18.467718  18.467718  110429460    4   \n",
       "3     18.404655   18.503002  18.282784  18.387136  18.387136   66161772    7   \n",
       "4     18.406906   18.425926  18.128880  18.350851  18.350851   66976956    8   \n",
       "...         ...         ...        ...        ...        ...        ...  ...   \n",
       "2531  97.949997  100.040001  97.500000  99.790001  99.790001   40005100   23   \n",
       "2532  98.099998   99.610001  97.199997  97.699997  97.699997   33078500   24   \n",
       "2533  95.570000   96.160004  93.760002  95.220001  95.220001   42330000   25   \n",
       "2534  96.500000   97.570000  95.379997  97.519997  97.519997   30114000   26   \n",
       "2535  97.309998  100.320000  97.309998  99.370003  99.370003   33850200   27   \n",
       "\n",
       "      month  year  \n",
       "0         1  2013  \n",
       "1         1  2013  \n",
       "2         1  2013  \n",
       "3         1  2013  \n",
       "4         1  2013  \n",
       "...     ...   ...  \n",
       "2531      1  2023  \n",
       "2532      1  2023  \n",
       "2533      1  2023  \n",
       "2534      1  2023  \n",
       "2535      1  2023  \n",
       "\n",
       "[2536 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "607ee0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "High         1.000000\n",
       "Open         0.999820\n",
       "Close        0.999799\n",
       "Adj Close    0.999799\n",
       "Low          0.999757\n",
       "year         0.900219\n",
       "month        0.055729\n",
       "day          0.000288\n",
       "Volume       0.328124\n",
       "Name: High, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(df.corr()[\"High\"].sort_values(ascending=False)) #We look at their correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af12f894",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b3be577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2536, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y=df[[\"year\",\"month\",\"Volume\"]],df[[\"High\"]]\n",
    "x=scaler.fit_transform(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30e2b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "281cb732",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(50,activation=\"relu\"))\n",
    "model.add(Dense(50,activation=\"relu\"))\n",
    "model.add(Dense(50,activation=\"relu\"))\n",
    "model.add(Dense(50,activation=\"relu\"))\n",
    "model.add(Dense(50,activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\",loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09506c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "203/203 [==============================] - 1s 2ms/step - loss: 1668.3286 - val_loss: 263.3529\n",
      "Epoch 2/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 218.2598 - val_loss: 198.5639\n",
      "Epoch 3/500\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 176.4422 - val_loss: 185.7209\n",
      "Epoch 4/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 157.8794 - val_loss: 149.6730\n",
      "Epoch 5/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 156.9680 - val_loss: 150.3144\n",
      "Epoch 6/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 152.3872 - val_loss: 147.4001\n",
      "Epoch 7/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 150.4949 - val_loss: 156.6055\n",
      "Epoch 8/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 155.0685 - val_loss: 146.7185\n",
      "Epoch 9/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 152.1902 - val_loss: 142.0171\n",
      "Epoch 10/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 148.8678 - val_loss: 138.2359\n",
      "Epoch 11/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 142.3015 - val_loss: 142.4150\n",
      "Epoch 12/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 141.0352 - val_loss: 134.7499\n",
      "Epoch 13/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 137.5672 - val_loss: 132.1077\n",
      "Epoch 14/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 150.7186 - val_loss: 143.7689\n",
      "Epoch 15/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 134.7106 - val_loss: 130.0031\n",
      "Epoch 16/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 142.9470 - val_loss: 130.9783\n",
      "Epoch 17/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 137.8839 - val_loss: 169.6033\n",
      "Epoch 18/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 132.2191 - val_loss: 121.6229\n",
      "Epoch 19/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 130.5835 - val_loss: 121.4368\n",
      "Epoch 20/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 128.0766 - val_loss: 174.8034\n",
      "Epoch 21/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 125.5252 - val_loss: 136.8008\n",
      "Epoch 22/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 130.8693 - val_loss: 134.9732\n",
      "Epoch 23/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 126.7958 - val_loss: 125.0176\n",
      "Epoch 24/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 127.1295 - val_loss: 115.6801\n",
      "Epoch 25/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 121.6619 - val_loss: 117.5791\n",
      "Epoch 26/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 126.8313 - val_loss: 110.6197\n",
      "Epoch 27/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 116.3273 - val_loss: 110.6194\n",
      "Epoch 28/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 117.6286 - val_loss: 115.8043\n",
      "Epoch 29/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 109.5344 - val_loss: 145.5964\n",
      "Epoch 30/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 112.4483 - val_loss: 124.9483\n",
      "Epoch 31/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 111.5756 - val_loss: 105.6018\n",
      "Epoch 32/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 104.9054 - val_loss: 104.4173\n",
      "Epoch 33/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 103.6521 - val_loss: 113.4392\n",
      "Epoch 34/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 108.3339 - val_loss: 143.7348\n",
      "Epoch 35/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 97.6653 - val_loss: 95.4405\n",
      "Epoch 36/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 101.4715 - val_loss: 97.4597\n",
      "Epoch 37/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 96.7643 - val_loss: 88.8503\n",
      "Epoch 38/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 98.0207 - val_loss: 105.8069\n",
      "Epoch 39/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 94.3354 - val_loss: 85.4939\n",
      "Epoch 40/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 89.1734 - val_loss: 83.2148\n",
      "Epoch 41/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 89.6747 - val_loss: 98.4039\n",
      "Epoch 42/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 85.6487 - val_loss: 82.4660\n",
      "Epoch 43/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 82.5862 - val_loss: 79.7032\n",
      "Epoch 44/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 84.1049 - val_loss: 82.8591\n",
      "Epoch 45/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 80.0264 - val_loss: 75.9868\n",
      "Epoch 46/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 77.9243 - val_loss: 86.9035\n",
      "Epoch 47/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 74.4799 - val_loss: 73.6636\n",
      "Epoch 48/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 71.0233 - val_loss: 79.4023\n",
      "Epoch 49/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 68.8530 - val_loss: 65.7132\n",
      "Epoch 50/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 68.6585 - val_loss: 64.4971\n",
      "Epoch 51/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 58.4678 - val_loss: 58.2761\n",
      "Epoch 52/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 59.8701 - val_loss: 79.8120\n",
      "Epoch 53/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 50.4072 - val_loss: 49.0081\n",
      "Epoch 54/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 46.5500 - val_loss: 50.3597\n",
      "Epoch 55/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 39.2564 - val_loss: 90.9691\n",
      "Epoch 56/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 46.1727 - val_loss: 69.5762\n",
      "Epoch 57/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 34.1871 - val_loss: 35.4327\n",
      "Epoch 58/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 29.5497 - val_loss: 40.9942\n",
      "Epoch 59/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 27.9711 - val_loss: 24.5692\n",
      "Epoch 60/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 23.2387 - val_loss: 22.1314\n",
      "Epoch 61/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 22.8170 - val_loss: 18.6410\n",
      "Epoch 62/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 18.7529 - val_loss: 22.1843\n",
      "Epoch 63/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 16.2596 - val_loss: 15.5251\n",
      "Epoch 64/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 14.5464 - val_loss: 12.6202\n",
      "Epoch 65/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 13.9977 - val_loss: 15.4297\n",
      "Epoch 66/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 18.8488 - val_loss: 17.7423\n",
      "Epoch 67/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 14.4155 - val_loss: 12.3512\n",
      "Epoch 68/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 12.8343 - val_loss: 13.3597\n",
      "Epoch 69/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 13.2248 - val_loss: 19.3805\n",
      "Epoch 70/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 14.5500 - val_loss: 9.6518\n",
      "Epoch 71/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 11.6792 - val_loss: 12.4760\n",
      "Epoch 72/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 11.4423 - val_loss: 17.3898\n",
      "Epoch 73/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 16.1861 - val_loss: 8.6393\n",
      "Epoch 74/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.9645 - val_loss: 9.5521\n",
      "Epoch 75/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 12.5824 - val_loss: 11.5651\n",
      "Epoch 76/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 12.9953 - val_loss: 8.4477\n",
      "Epoch 77/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 15.3724 - val_loss: 10.0020\n",
      "Epoch 78/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 11.0883 - val_loss: 9.7439\n",
      "Epoch 79/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 10.4745 - val_loss: 14.1662\n",
      "Epoch 80/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 17.0928 - val_loss: 9.4559\n",
      "Epoch 81/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 10.1251 - val_loss: 12.7573\n",
      "Epoch 82/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 14.4496 - val_loss: 10.4387\n",
      "Epoch 83/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.8488 - val_loss: 10.0352\n",
      "Epoch 84/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 14.6445 - val_loss: 8.8113\n",
      "Epoch 85/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.0208 - val_loss: 13.4225\n",
      "Epoch 86/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 12.4839 - val_loss: 10.2621\n",
      "Epoch 87/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 11.2726 - val_loss: 7.6990\n",
      "Epoch 88/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 13.9149 - val_loss: 8.9314\n",
      "Epoch 89/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 12.6020 - val_loss: 9.2983\n",
      "Epoch 90/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.5259 - val_loss: 8.2184\n",
      "Epoch 91/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.8194 - val_loss: 15.5529\n",
      "Epoch 92/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 14.5440 - val_loss: 12.9234\n",
      "Epoch 93/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.2707 - val_loss: 16.3770\n",
      "Epoch 94/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 10.7362 - val_loss: 8.5677\n",
      "Epoch 95/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.6543 - val_loss: 8.8334\n",
      "Epoch 96/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.5944 - val_loss: 10.9609\n",
      "Epoch 97/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 14.1484 - val_loss: 9.2559\n",
      "Epoch 98/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 10.8367 - val_loss: 7.7925\n",
      "Epoch 99/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 10.9283 - val_loss: 7.6414\n",
      "Epoch 100/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 10.8223 - val_loss: 9.6252\n",
      "Epoch 101/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.5447 - val_loss: 8.7849\n",
      "Epoch 102/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.4509 - val_loss: 15.2306\n",
      "Epoch 103/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 12.0594 - val_loss: 7.1809\n",
      "Epoch 104/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.5788 - val_loss: 13.7383\n",
      "Epoch 105/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 10.0153 - val_loss: 13.1903\n",
      "Epoch 106/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.0824 - val_loss: 14.9791\n",
      "Epoch 107/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 10.5115 - val_loss: 8.2346\n",
      "Epoch 108/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.9531 - val_loss: 10.6500\n",
      "Epoch 109/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 16.3085 - val_loss: 6.7962\n",
      "Epoch 110/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.9393 - val_loss: 7.1182\n",
      "Epoch 111/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.9231 - val_loss: 7.3972\n",
      "Epoch 112/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.2915 - val_loss: 7.9444\n",
      "Epoch 113/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.9012 - val_loss: 11.6297\n",
      "Epoch 114/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 10.2316 - val_loss: 14.6964\n",
      "Epoch 115/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 12.3727 - val_loss: 8.2307\n",
      "Epoch 116/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.6226 - val_loss: 6.7831\n",
      "Epoch 117/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.1627 - val_loss: 6.7230\n",
      "Epoch 118/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.5046 - val_loss: 15.6833\n",
      "Epoch 119/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 10.6486 - val_loss: 9.1319\n",
      "Epoch 120/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.6661 - val_loss: 10.1635\n",
      "Epoch 121/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 10.1407 - val_loss: 6.1518\n",
      "Epoch 122/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.8582 - val_loss: 9.4101\n",
      "Epoch 123/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 10.0585 - val_loss: 8.1529\n",
      "Epoch 124/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.4410 - val_loss: 7.5129\n",
      "Epoch 125/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.7204 - val_loss: 9.8555\n",
      "Epoch 126/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 10.3457 - val_loss: 10.4698\n",
      "Epoch 127/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.3746 - val_loss: 10.9692\n",
      "Epoch 128/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.4524 - val_loss: 8.1566\n",
      "Epoch 129/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 11.0762 - val_loss: 9.0293\n",
      "Epoch 130/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 12.0354 - val_loss: 15.8935\n",
      "Epoch 131/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.2649 - val_loss: 9.8034\n",
      "Epoch 132/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 10.6856 - val_loss: 8.1691\n",
      "Epoch 133/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.9492 - val_loss: 10.7289\n",
      "Epoch 134/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.6421 - val_loss: 12.1872\n",
      "Epoch 135/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.6947 - val_loss: 9.8428\n",
      "Epoch 136/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.5786 - val_loss: 5.9718\n",
      "Epoch 137/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 11.9599 - val_loss: 10.2480\n",
      "Epoch 138/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 11.2252 - val_loss: 9.5857\n",
      "Epoch 139/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.0538 - val_loss: 6.2340\n",
      "Epoch 140/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.1174 - val_loss: 7.4230\n",
      "Epoch 141/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.8792 - val_loss: 6.3240\n",
      "Epoch 142/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.1319 - val_loss: 9.4509\n",
      "Epoch 143/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.7694 - val_loss: 6.1190\n",
      "Epoch 144/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.8142 - val_loss: 8.1754\n",
      "Epoch 145/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.2604 - val_loss: 12.9867\n",
      "Epoch 146/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.4645 - val_loss: 9.9816\n",
      "Epoch 147/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.9681 - val_loss: 6.5627\n",
      "Epoch 148/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.6458 - val_loss: 8.7343\n",
      "Epoch 149/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.6889 - val_loss: 16.5952\n",
      "Epoch 150/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.8549 - val_loss: 9.1861\n",
      "Epoch 151/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.9867 - val_loss: 10.6146\n",
      "Epoch 152/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.2056 - val_loss: 8.5626\n",
      "Epoch 153/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.2483 - val_loss: 6.0288\n",
      "Epoch 154/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.1176 - val_loss: 7.6523\n",
      "Epoch 155/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.7010 - val_loss: 8.0489\n",
      "Epoch 156/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 10.7813 - val_loss: 10.3345\n",
      "Epoch 157/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.2461 - val_loss: 7.7740\n",
      "Epoch 158/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203/203 [==============================] - 0s 1ms/step - loss: 8.0139 - val_loss: 8.2216\n",
      "Epoch 159/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.4821 - val_loss: 8.1186\n",
      "Epoch 160/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.1485 - val_loss: 7.5161\n",
      "Epoch 161/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.9308 - val_loss: 22.1572\n",
      "Epoch 162/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 11.6099 - val_loss: 6.5536\n",
      "Epoch 163/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 10.0249 - val_loss: 7.8511\n",
      "Epoch 164/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.5732 - val_loss: 7.3961\n",
      "Epoch 165/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.4723 - val_loss: 5.7201\n",
      "Epoch 166/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.1669 - val_loss: 6.7346\n",
      "Epoch 167/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 11.1972 - val_loss: 8.1187\n",
      "Epoch 168/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.5086 - val_loss: 6.2764\n",
      "Epoch 169/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.3046 - val_loss: 7.3883\n",
      "Epoch 170/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.7877 - val_loss: 9.9898\n",
      "Epoch 171/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.1158 - val_loss: 7.2009\n",
      "Epoch 172/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.8043 - val_loss: 8.7827\n",
      "Epoch 173/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.3397 - val_loss: 9.0950\n",
      "Epoch 174/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 10.1757 - val_loss: 14.2707\n",
      "Epoch 175/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.6636 - val_loss: 7.7319\n",
      "Epoch 176/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.4158 - val_loss: 6.1999\n",
      "Epoch 177/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.2459 - val_loss: 8.0749\n",
      "Epoch 178/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.8112 - val_loss: 7.5287\n",
      "Epoch 179/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.5242 - val_loss: 14.6418\n",
      "Epoch 180/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.0212 - val_loss: 9.3188\n",
      "Epoch 181/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.0926 - val_loss: 8.5125\n",
      "Epoch 182/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.3923 - val_loss: 5.8153\n",
      "Epoch 183/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 11.6067 - val_loss: 6.7638\n",
      "Epoch 184/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.5000 - val_loss: 10.6529\n",
      "Epoch 185/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.8497 - val_loss: 7.4632\n",
      "Epoch 186/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.2354 - val_loss: 6.8808\n",
      "Epoch 187/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.1736 - val_loss: 5.4201\n",
      "Epoch 188/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.3408 - val_loss: 9.0543\n",
      "Epoch 189/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.5489 - val_loss: 6.5004\n",
      "Epoch 190/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.7219 - val_loss: 7.6467\n",
      "Epoch 191/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.7062 - val_loss: 10.8237\n",
      "Epoch 192/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.0908 - val_loss: 5.9326\n",
      "Epoch 193/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.3584 - val_loss: 6.5609\n",
      "Epoch 194/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.9925 - val_loss: 5.0197\n",
      "Epoch 195/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.1610 - val_loss: 6.0461\n",
      "Epoch 196/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.7923 - val_loss: 7.2107\n",
      "Epoch 197/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.2790 - val_loss: 7.0362\n",
      "Epoch 198/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.0142 - val_loss: 5.7622\n",
      "Epoch 199/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.1990 - val_loss: 10.2589\n",
      "Epoch 200/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.5086 - val_loss: 5.1640\n",
      "Epoch 201/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.3873 - val_loss: 6.6178\n",
      "Epoch 202/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.6930 - val_loss: 8.0558\n",
      "Epoch 203/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.3222 - val_loss: 7.8634\n",
      "Epoch 204/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.2986 - val_loss: 5.7165\n",
      "Epoch 205/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.2719 - val_loss: 6.4591\n",
      "Epoch 206/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.6313 - val_loss: 5.6006\n",
      "Epoch 207/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.9270 - val_loss: 8.5467\n",
      "Epoch 208/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.9237 - val_loss: 9.8292\n",
      "Epoch 209/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.1591 - val_loss: 5.0482\n",
      "Epoch 210/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.7189 - val_loss: 5.8679\n",
      "Epoch 211/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.3438 - val_loss: 5.5037\n",
      "Epoch 212/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.0687 - val_loss: 5.3932\n",
      "Epoch 213/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.1126 - val_loss: 8.1544\n",
      "Epoch 214/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.5584 - val_loss: 11.4213\n",
      "Epoch 215/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.9393 - val_loss: 6.3244\n",
      "Epoch 216/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.1774 - val_loss: 6.2216\n",
      "Epoch 217/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.2732 - val_loss: 5.4495\n",
      "Epoch 218/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.6464 - val_loss: 7.0649\n",
      "Epoch 219/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.5948 - val_loss: 6.2055\n",
      "Epoch 220/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.1691 - val_loss: 8.3479\n",
      "Epoch 221/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.9229 - val_loss: 8.0061\n",
      "Epoch 222/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.1832 - val_loss: 5.5942\n",
      "Epoch 223/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.1306 - val_loss: 8.8125\n",
      "Epoch 224/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.4937 - val_loss: 5.6537\n",
      "Epoch 225/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.4448 - val_loss: 6.4977\n",
      "Epoch 226/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.0855 - val_loss: 6.7356\n",
      "Epoch 227/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.9129 - val_loss: 6.0065\n",
      "Epoch 228/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.7022 - val_loss: 8.9568\n",
      "Epoch 229/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.1539 - val_loss: 10.4219\n",
      "Epoch 230/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.2254 - val_loss: 5.4936\n",
      "Epoch 231/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.6619 - val_loss: 5.5872\n",
      "Epoch 232/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.2644 - val_loss: 5.6667\n",
      "Epoch 233/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.6894 - val_loss: 8.5099\n",
      "Epoch 234/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.2480 - val_loss: 7.4941\n",
      "Epoch 235/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 14.7671 - val_loss: 15.4334\n",
      "Epoch 236/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.9198 - val_loss: 7.4556\n",
      "Epoch 237/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.2644 - val_loss: 6.2597\n",
      "Epoch 238/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.5471 - val_loss: 5.8888\n",
      "Epoch 239/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.9063 - val_loss: 8.6884\n",
      "Epoch 240/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.7411 - val_loss: 6.0917\n",
      "Epoch 241/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.5042 - val_loss: 7.7009\n",
      "Epoch 242/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.0734 - val_loss: 5.9217\n",
      "Epoch 243/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.1037 - val_loss: 6.2110\n",
      "Epoch 244/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.6482 - val_loss: 6.2428\n",
      "Epoch 245/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.0990 - val_loss: 5.9549\n",
      "Epoch 246/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.9215 - val_loss: 9.9826\n",
      "Epoch 247/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.9704 - val_loss: 5.2554\n",
      "Epoch 248/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.0986 - val_loss: 6.3460\n",
      "Epoch 249/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.1955 - val_loss: 5.4738\n",
      "Epoch 250/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.2410 - val_loss: 5.7331\n",
      "Epoch 251/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.8998 - val_loss: 6.5404\n",
      "Epoch 252/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.8262 - val_loss: 5.9030\n",
      "Epoch 253/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.3895 - val_loss: 6.3704\n",
      "Epoch 254/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.7536 - val_loss: 5.1375\n",
      "Epoch 255/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.9556 - val_loss: 9.0830\n",
      "Epoch 256/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.2545 - val_loss: 8.7652\n",
      "Epoch 257/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.4239 - val_loss: 5.3393\n",
      "Epoch 258/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.1965 - val_loss: 5.4997\n",
      "Epoch 259/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.6930 - val_loss: 5.2689\n",
      "Epoch 260/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.7743 - val_loss: 6.5456\n",
      "Epoch 261/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.1768 - val_loss: 5.4149\n",
      "Epoch 262/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.0353 - val_loss: 9.7393\n",
      "Epoch 263/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.2959 - val_loss: 5.1097\n",
      "Epoch 264/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.9348 - val_loss: 5.7465\n",
      "Epoch 265/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.2208 - val_loss: 6.0598\n",
      "Epoch 266/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.9812 - val_loss: 6.0846\n",
      "Epoch 267/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.7623 - val_loss: 24.5074\n",
      "Epoch 268/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.8947 - val_loss: 5.9816\n",
      "Epoch 269/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.0926 - val_loss: 7.0650\n",
      "Epoch 270/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.5472 - val_loss: 5.5981\n",
      "Epoch 271/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.9943 - val_loss: 6.8933\n",
      "Epoch 272/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.3163 - val_loss: 10.6164\n",
      "Epoch 273/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.1497 - val_loss: 5.5819\n",
      "Epoch 274/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.5691 - val_loss: 7.0695\n",
      "Epoch 275/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.9774 - val_loss: 5.8306\n",
      "Epoch 276/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.6939 - val_loss: 6.3469\n",
      "Epoch 277/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.9821 - val_loss: 6.9017\n",
      "Epoch 278/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.9156 - val_loss: 5.6855\n",
      "Epoch 279/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.7473 - val_loss: 22.6216\n",
      "Epoch 280/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.5190 - val_loss: 4.6572\n",
      "Epoch 281/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.4426 - val_loss: 6.4842\n",
      "Epoch 282/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.1034 - val_loss: 8.2172\n",
      "Epoch 283/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.9318 - val_loss: 5.5227\n",
      "Epoch 284/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.2663 - val_loss: 5.8932\n",
      "Epoch 285/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.3725 - val_loss: 12.1663\n",
      "Epoch 286/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.7684 - val_loss: 4.8367\n",
      "Epoch 287/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.0805 - val_loss: 7.4951\n",
      "Epoch 288/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.8866 - val_loss: 11.8019\n",
      "Epoch 289/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.7318 - val_loss: 8.0641\n",
      "Epoch 290/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.3658 - val_loss: 4.9305\n",
      "Epoch 291/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.8194 - val_loss: 9.2887\n",
      "Epoch 292/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.5704 - val_loss: 5.4915\n",
      "Epoch 293/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.0346 - val_loss: 5.7462\n",
      "Epoch 294/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.3326 - val_loss: 24.7366\n",
      "Epoch 295/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.6542 - val_loss: 5.2852\n",
      "Epoch 296/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.2150 - val_loss: 4.7163\n",
      "Epoch 297/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.5257 - val_loss: 4.7819\n",
      "Epoch 298/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.8140 - val_loss: 5.9414\n",
      "Epoch 299/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.4606 - val_loss: 6.8078\n",
      "Epoch 300/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.2855 - val_loss: 5.0862\n",
      "Epoch 301/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.8688 - val_loss: 6.2796\n",
      "Epoch 302/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.9959 - val_loss: 5.5378\n",
      "Epoch 303/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.4419 - val_loss: 7.7578\n",
      "Epoch 304/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.3654 - val_loss: 5.4313\n",
      "Epoch 305/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.1044 - val_loss: 8.4026\n",
      "Epoch 306/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.9874 - val_loss: 5.2324\n",
      "Epoch 307/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.5244 - val_loss: 5.9037\n",
      "Epoch 308/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.7506 - val_loss: 4.9117\n",
      "Epoch 309/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1862 - val_loss: 5.7646\n",
      "Epoch 310/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.3698 - val_loss: 6.2475\n",
      "Epoch 311/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.0660 - val_loss: 5.6604\n",
      "Epoch 312/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1061 - val_loss: 8.8995\n",
      "Epoch 313/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.7662 - val_loss: 6.0263\n",
      "Epoch 314/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.3502 - val_loss: 6.1742\n",
      "Epoch 315/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.3703 - val_loss: 4.8329\n",
      "Epoch 316/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203/203 [==============================] - 0s 1ms/step - loss: 6.2120 - val_loss: 6.6815\n",
      "Epoch 317/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.7095 - val_loss: 5.3023\n",
      "Epoch 318/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.0530 - val_loss: 4.8045\n",
      "Epoch 319/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.5864 - val_loss: 9.6084\n",
      "Epoch 320/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.0324 - val_loss: 5.8487\n",
      "Epoch 321/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.0467 - val_loss: 5.2645\n",
      "Epoch 322/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.2101 - val_loss: 5.6665\n",
      "Epoch 323/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.5710 - val_loss: 5.3790\n",
      "Epoch 324/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.7842 - val_loss: 11.6591\n",
      "Epoch 325/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.3540 - val_loss: 6.7214\n",
      "Epoch 326/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.0938 - val_loss: 8.2815\n",
      "Epoch 327/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.2632 - val_loss: 5.5390\n",
      "Epoch 328/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1371 - val_loss: 8.7400\n",
      "Epoch 329/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.8363 - val_loss: 5.1808\n",
      "Epoch 330/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1616 - val_loss: 5.4942\n",
      "Epoch 331/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1310 - val_loss: 8.8436\n",
      "Epoch 332/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.3472 - val_loss: 4.5945\n",
      "Epoch 333/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.5973 - val_loss: 4.6159\n",
      "Epoch 334/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.4699 - val_loss: 5.0475\n",
      "Epoch 335/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1326 - val_loss: 7.3342\n",
      "Epoch 336/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.2318 - val_loss: 6.6036\n",
      "Epoch 337/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1889 - val_loss: 6.5267\n",
      "Epoch 338/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.0151 - val_loss: 6.6647\n",
      "Epoch 339/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.7208 - val_loss: 4.9407\n",
      "Epoch 340/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.6319 - val_loss: 5.1224\n",
      "Epoch 341/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.3685 - val_loss: 7.4514\n",
      "Epoch 342/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.5214 - val_loss: 5.7421\n",
      "Epoch 343/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.4684 - val_loss: 4.9347\n",
      "Epoch 344/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.8186 - val_loss: 4.9118\n",
      "Epoch 345/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.2187 - val_loss: 9.6066\n",
      "Epoch 346/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.9763 - val_loss: 5.9756\n",
      "Epoch 347/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1982 - val_loss: 5.7888\n",
      "Epoch 348/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.5759 - val_loss: 7.0752\n",
      "Epoch 349/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.5236 - val_loss: 7.5445\n",
      "Epoch 350/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.8126 - val_loss: 17.3158\n",
      "Epoch 351/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.3869 - val_loss: 5.8298\n",
      "Epoch 352/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.9209 - val_loss: 9.1372\n",
      "Epoch 353/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.9331 - val_loss: 5.1374\n",
      "Epoch 354/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.4257 - val_loss: 5.5482\n",
      "Epoch 355/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.4424 - val_loss: 5.6988\n",
      "Epoch 356/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.2851 - val_loss: 5.1009\n",
      "Epoch 357/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.4654 - val_loss: 7.0193\n",
      "Epoch 358/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.4414 - val_loss: 6.7196\n",
      "Epoch 359/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.6891 - val_loss: 6.8023\n",
      "Epoch 360/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.7443 - val_loss: 17.7114\n",
      "Epoch 361/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.9349 - val_loss: 9.1862\n",
      "Epoch 362/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.7213 - val_loss: 4.6665\n",
      "Epoch 363/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.9543 - val_loss: 7.5681\n",
      "Epoch 364/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.2836 - val_loss: 4.9740\n",
      "Epoch 365/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.0611 - val_loss: 5.2145\n",
      "Epoch 366/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.2530 - val_loss: 4.9527\n",
      "Epoch 367/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.4390 - val_loss: 5.0802\n",
      "Epoch 368/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.6597 - val_loss: 8.1357\n",
      "Epoch 369/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.8430 - val_loss: 7.0504\n",
      "Epoch 370/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.9895 - val_loss: 9.9276\n",
      "Epoch 371/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.5733 - val_loss: 6.4806\n",
      "Epoch 372/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.9801 - val_loss: 4.9601\n",
      "Epoch 373/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.4536 - val_loss: 13.6149\n",
      "Epoch 374/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 11.5506 - val_loss: 5.4515\n",
      "Epoch 375/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.8333 - val_loss: 6.1659\n",
      "Epoch 376/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.7249 - val_loss: 4.5373\n",
      "Epoch 377/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.8539 - val_loss: 5.6090\n",
      "Epoch 378/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.6536 - val_loss: 4.6726\n",
      "Epoch 379/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.5140 - val_loss: 4.9280\n",
      "Epoch 380/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.0934 - val_loss: 5.6696\n",
      "Epoch 381/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.2821 - val_loss: 5.4435\n",
      "Epoch 382/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.6800 - val_loss: 5.0196\n",
      "Epoch 383/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.5140 - val_loss: 7.0256\n",
      "Epoch 384/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.1783 - val_loss: 6.6744\n",
      "Epoch 385/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1403 - val_loss: 8.3683\n",
      "Epoch 386/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.6986 - val_loss: 5.5486\n",
      "Epoch 387/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.1567 - val_loss: 7.1698\n",
      "Epoch 388/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.3089 - val_loss: 6.8550\n",
      "Epoch 389/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1086 - val_loss: 4.7110\n",
      "Epoch 390/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.6278 - val_loss: 5.4464\n",
      "Epoch 391/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.0402 - val_loss: 5.5070\n",
      "Epoch 392/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.2256 - val_loss: 5.8321\n",
      "Epoch 393/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.5854 - val_loss: 25.9782\n",
      "Epoch 394/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.2829 - val_loss: 4.7552\n",
      "Epoch 395/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.8632 - val_loss: 9.5984\n",
      "Epoch 396/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1556 - val_loss: 8.6833\n",
      "Epoch 397/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.4383 - val_loss: 6.1027\n",
      "Epoch 398/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.6949 - val_loss: 4.8008\n",
      "Epoch 399/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.9884 - val_loss: 5.1808\n",
      "Epoch 400/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.2011 - val_loss: 7.8127\n",
      "Epoch 401/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.9373 - val_loss: 7.0607\n",
      "Epoch 402/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.3379 - val_loss: 6.8181\n",
      "Epoch 403/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.6006 - val_loss: 5.9857\n",
      "Epoch 404/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.8675 - val_loss: 4.6805\n",
      "Epoch 405/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.6803 - val_loss: 5.1899\n",
      "Epoch 406/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.0674 - val_loss: 8.6030\n",
      "Epoch 407/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.1861 - val_loss: 13.5194\n",
      "Epoch 408/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.2544 - val_loss: 8.2652\n",
      "Epoch 409/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.7318 - val_loss: 7.2829\n",
      "Epoch 410/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.9185 - val_loss: 5.3406\n",
      "Epoch 411/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.0638 - val_loss: 4.9332\n",
      "Epoch 412/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.4483 - val_loss: 6.3448\n",
      "Epoch 413/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1192 - val_loss: 5.0800\n",
      "Epoch 414/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.3208 - val_loss: 5.0009\n",
      "Epoch 415/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.0758 - val_loss: 4.6632\n",
      "Epoch 416/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.9554 - val_loss: 7.6245\n",
      "Epoch 417/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1174 - val_loss: 5.5882\n",
      "Epoch 418/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.9923 - val_loss: 4.4595\n",
      "Epoch 419/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.5459 - val_loss: 11.0251\n",
      "Epoch 420/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.0050 - val_loss: 4.7460\n",
      "Epoch 421/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.9408 - val_loss: 4.6750\n",
      "Epoch 422/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.0799 - val_loss: 8.2032\n",
      "Epoch 423/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.0012 - val_loss: 4.5554\n",
      "Epoch 424/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.5117 - val_loss: 7.4643\n",
      "Epoch 425/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.5275 - val_loss: 7.4119\n",
      "Epoch 426/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.2192 - val_loss: 8.5018\n",
      "Epoch 427/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.3641 - val_loss: 5.7986\n",
      "Epoch 428/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.5378 - val_loss: 5.0549\n",
      "Epoch 429/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.5436 - val_loss: 6.0934\n",
      "Epoch 430/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1361 - val_loss: 5.2687\n",
      "Epoch 431/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.1033 - val_loss: 5.2190\n",
      "Epoch 432/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.0447 - val_loss: 6.1284\n",
      "Epoch 433/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.6404 - val_loss: 5.5674\n",
      "Epoch 434/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.6506 - val_loss: 6.7540\n",
      "Epoch 435/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.0171 - val_loss: 4.3216\n",
      "Epoch 436/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.9970 - val_loss: 5.9314\n",
      "Epoch 437/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.6965 - val_loss: 4.9290\n",
      "Epoch 438/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.2032 - val_loss: 4.8021\n",
      "Epoch 439/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1506 - val_loss: 6.5743\n",
      "Epoch 440/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.0572 - val_loss: 12.6011\n",
      "Epoch 441/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.2938 - val_loss: 7.1211\n",
      "Epoch 442/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.2186 - val_loss: 6.3749\n",
      "Epoch 443/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.4511 - val_loss: 6.1943\n",
      "Epoch 444/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.5251 - val_loss: 4.6836\n",
      "Epoch 445/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.9913 - val_loss: 5.5806\n",
      "Epoch 446/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.7084 - val_loss: 5.6163\n",
      "Epoch 447/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.9907 - val_loss: 4.6519\n",
      "Epoch 448/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.4795 - val_loss: 5.9006\n",
      "Epoch 449/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.2894 - val_loss: 5.2095\n",
      "Epoch 450/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.9827 - val_loss: 6.3329\n",
      "Epoch 451/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1582 - val_loss: 6.6621\n",
      "Epoch 452/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1725 - val_loss: 5.4169\n",
      "Epoch 453/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.9342 - val_loss: 5.2396\n",
      "Epoch 454/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.6370 - val_loss: 10.4867\n",
      "Epoch 455/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.6143 - val_loss: 5.9906\n",
      "Epoch 456/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 12.4296 - val_loss: 7.2627\n",
      "Epoch 457/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.9948 - val_loss: 4.4955\n",
      "Epoch 458/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.0651 - val_loss: 5.0652\n",
      "Epoch 459/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.4450 - val_loss: 4.6035\n",
      "Epoch 460/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.4653 - val_loss: 6.7068\n",
      "Epoch 461/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.2337 - val_loss: 4.9509\n",
      "Epoch 462/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.8145 - val_loss: 5.4401\n",
      "Epoch 463/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.6959 - val_loss: 5.1179\n",
      "Epoch 464/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.0409 - val_loss: 5.0308\n",
      "Epoch 465/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.8313 - val_loss: 5.5712\n",
      "Epoch 466/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.9638 - val_loss: 6.7683\n",
      "Epoch 467/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.4158 - val_loss: 5.2138\n",
      "Epoch 468/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.9900 - val_loss: 7.0332\n",
      "Epoch 469/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.0892 - val_loss: 7.3243\n",
      "Epoch 470/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.2874 - val_loss: 5.4620\n",
      "Epoch 471/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.7944 - val_loss: 4.5885\n",
      "Epoch 472/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1114 - val_loss: 5.1127\n",
      "Epoch 473/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.4072 - val_loss: 5.0304\n",
      "Epoch 474/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203/203 [==============================] - 0s 1ms/step - loss: 5.2189 - val_loss: 8.3048\n",
      "Epoch 475/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.7668 - val_loss: 5.3026\n",
      "Epoch 476/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.9709 - val_loss: 4.9904\n",
      "Epoch 477/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.2650 - val_loss: 5.4833\n",
      "Epoch 478/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.5922 - val_loss: 9.6033\n",
      "Epoch 479/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1660 - val_loss: 6.5702\n",
      "Epoch 480/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.6719 - val_loss: 4.8660\n",
      "Epoch 481/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.8594 - val_loss: 5.3770\n",
      "Epoch 482/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.4573 - val_loss: 6.1909\n",
      "Epoch 483/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.8249 - val_loss: 5.1882\n",
      "Epoch 484/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.2135 - val_loss: 4.6149\n",
      "Epoch 485/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.0718 - val_loss: 5.1346\n",
      "Epoch 486/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.9545 - val_loss: 5.4732\n",
      "Epoch 487/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.9627 - val_loss: 9.9288\n",
      "Epoch 488/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.0885 - val_loss: 8.1664\n",
      "Epoch 489/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.2265 - val_loss: 5.1645\n",
      "Epoch 490/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.9707 - val_loss: 5.9862\n",
      "Epoch 491/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.5064 - val_loss: 18.6866\n",
      "Epoch 492/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.5386 - val_loss: 6.3118\n",
      "Epoch 493/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.5376 - val_loss: 4.9654\n",
      "Epoch 494/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.0711 - val_loss: 7.8472\n",
      "Epoch 495/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.6467 - val_loss: 5.6693\n",
      "Epoch 496/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.8671 - val_loss: 7.9020\n",
      "Epoch 497/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.8117 - val_loss: 4.7333\n",
      "Epoch 498/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.5314 - val_loss: 5.4965\n",
      "Epoch 499/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.4560 - val_loss: 4.8514\n",
      "Epoch 500/500\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.8819 - val_loss: 5.6669\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 50)                200       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,451\n",
      "Trainable params: 10,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),batch_size=10,epochs=500)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "302553e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 932us/step\n"
     ]
    }
   ],
   "source": [
    "tahmin=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50770bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.995601923849578"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(tahmin,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6abec08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3805249300567337"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mean_squared_error(tahmin,y_test))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d979b68e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
